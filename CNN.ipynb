{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16ab59379b98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.finance import candlestick2_ohlc\n",
    "import os\n",
    "from PIL import Image\n",
    "import talib\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, add\n",
    "from keras.layers import Dropout, Flatten, LSTM, ReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "#%%\n",
    "raw_data = pd.read_excel('audusd.xlsx', index_col =0, names = ['date', 'open', 'high', 'low', 'close'],\\\n",
    "                         dtype = {'date':'str'}, usecols = [2, 3, 4, 5, 6])\n",
    "data = raw_data.loc['2000-01-01':]\n",
    "train_number = len(data.loc[:'2018-01-01'])\n",
    "\n",
    "#%%\n",
    "# Add moving average indicators\n",
    "data['sma5'] = talib.SMA(data['close'], timeperiod=5)\n",
    "data['sma10'] = talib.SMA(data['close'], timeperiod=10)\n",
    "data['sma15'] = talib.SMA(data['close'], timeperiod=15)\n",
    "data['sma20'] = talib.SMA(data['close'], timeperiod=20)\n",
    "data['sma25'] = talib.SMA(data['close'], timeperiod=25)\n",
    "data = data.dropna()\n",
    "data = data.round({'sma5': 3, 'sma10': 3, 'sma15': 3, 'sma20': 3, 'sma25': 3})\n",
    "#%%\n",
    "data.loc[(data['sma5']==data['sma10']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma5']==data['sma15']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma5']==data['sma20']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma5']==data['sma25']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma10']==data['sma15']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma10']==data['sma20']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma10']==data['sma25']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma15']==data['sma20']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma15']==data['sma25']), 'highly_predict'] = 1\n",
    "data.loc[(data['sma20']==data['sma25']), 'highly_predict'] = 1\n",
    "#%%\n",
    "# If you have not created picture,please create firstly.\n",
    "# Create caddlestick picture\n",
    "def caddlestick_data(data, days_for_train, days_for_label, p, highly_predict=False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        data: contain open, high, low, close.\n",
    "        days_for_train: window size for training.\n",
    "        days_for_label: window size for label.\n",
    "        p: the up percentage for seperating label 1 and 0.\n",
    "        highly_predict: True is to only create highly predicted images.\n",
    "    return: caddelstick image dataset.\n",
    "    \"\"\"\n",
    "    if highly_predict:\n",
    "        highpred_list = data[days_for_train:-days_for_label].loc[data['highly_predict']==1].index\n",
    "        for i,d in enumerate(highpred_list):\n",
    "            c = data.ix[:d][-days_for_train:]\n",
    "            plt.style.use('dark_background')\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "        \n",
    "            ax.plot(c['sma5'].values, linewidth=2.2)\n",
    "            ax.plot(c['sma10'].values, linewidth=2.2)\n",
    "            ax.plot(c['sma15'].values, linewidth=2.2)\n",
    "            ax.plot(c['sma20'].values, linewidth=2.2)\n",
    "        \n",
    "        \n",
    "            candlestick2_ohlc(ax, c['open'], c['high'], c['low'], c['close'] , width = 0.8, colorup = 'g', colordown = 'r')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            l = data.ix[d:][:days_for_label+1]\n",
    "            change = ((l['close'].shift(-1)-l['close'])/l['close']).dropna()   \n",
    "    \n",
    "            if any(e > p for e in change):\n",
    "                pngfile = '{}-{}_{}.png'.format('h', '%05d'%i, 1)\n",
    "                fig.savefig(pngfile, bbox_inches='tight',pad_inches = 0)\n",
    "            else:\n",
    "                pngfile = '{}-{}_{}.png'.format('h', '%05d'%i, 0)\n",
    "                fig.savefig(pngfile, bbox_inches='tight',pad_inches = 0)\n",
    "            plt.close(fig)\n",
    "    \n",
    "    else:\n",
    "        for i in range(0, len(data) - days_for_train - days_for_label - 1):\n",
    "            c = data.ix[i:i + int(days_for_train), :]\n",
    "            plt.style.use('dark_background')\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "        \n",
    "            ax.plot(c['sma5'].values, linewidth=1.8)\n",
    "            ax.plot(c['sma10'].values, linewidth=1.8)\n",
    "            ax.plot(c['sma15'].values, linewidth=1.8)\n",
    "            ax.plot(c['sma20'].values, linewidth=1.8)\n",
    "        \n",
    "        \n",
    "            candlestick2_ohlc(ax, c['open'], c['high'], c['low'], c['close'] , width = 0.5, colorup = 'g', colordown = 'r')\n",
    "            plt.axis('off')\n",
    "        \n",
    "            change = []\n",
    "            for j in range(1, days_for_label+1):\n",
    "                change.append((data['close'][i + int(days_for_train) + j] - \\\n",
    "                           data['close'][i + int(days_for_train)]) /data['close'][i + int(days_for_train)])\n",
    "    \n",
    "            if any(e > p for e in change):\n",
    "                pngfile = '{}_{}.png'.format('%05d'%i, 1)\n",
    "                fig.savefig(pngfile, bbox_inches='tight',pad_inches = 0)\n",
    "            else:\n",
    "                pngfile = '{}_{}.png'.format('%05d'%i, 0)\n",
    "                fig.savefig(pngfile, bbox_inches='tight',pad_inches = 0)\n",
    "            plt.close(fig)\n",
    "    print(\"Finish Creating Picture.\")\n",
    "    \n",
    "#%%\n",
    "#Save picture\n",
    "days_for_train = 20\n",
    "days_for_label = 5\n",
    "p = 0.01\n",
    "\n",
    "#create image data\n",
    "caddlestick_data(data, days_for_train, days_for_label, p)\n",
    "\n",
    "#create highly-predicted image data\n",
    "#caddlestick_data(data, days_for_train, days_for_label, p, highly_predict=True)\n",
    "\n",
    "#%%\n",
    "def get_dataset(width, hight, highly_predict=False):\n",
    "    image = []\n",
    "    label = []\n",
    "    index = []\n",
    "    \n",
    "    if highly_predict:\n",
    "        for pic in os.listdir('.'):\n",
    "            if pic.endswith('.png') and pic.split(\"-\")[0]=='h':\n",
    "                Im = Image.open(pic)\n",
    "                Im = np.array(Im.resize((width,hight),Image.NEAREST).convert('RGB'))\n",
    "                image.append(Im)\n",
    "                label.append(int(pic.split(\"-\")[1].split(\"_\")[1].split(\".\")[0]))\n",
    "                index.append(int(pic.split(\"-\")[1].split(\"_\")[0]))    \n",
    "        \n",
    "    else:\n",
    "        for pic in os.listdir('.'):\n",
    "            if pic.endswith('.png') and pic.split(\"-\")[0]!='h':\n",
    "                Im = Image.open(pic)\n",
    "                Im = np.array(Im.resize((width,hight),Image.NEAREST).convert('RGB'))\n",
    "                image.append(Im)\n",
    "                label.append(int(pic.split(\"_\")[1].split(\".\")[0]))\n",
    "                index.append(int(pic.split(\"_\")[0]))\n",
    "                \n",
    "    label = np.array(label)\n",
    "    image = np.array(image)\n",
    "    index = np.array(index)\n",
    "    #dataset = pd.DataFrame({'index': index,'image': image,'label': label})\n",
    "    return index, image, label\n",
    "\n",
    "#%%\n",
    "#load data\n",
    "index, image, label = get_dataset(128, 128, highly_predict=False)\n",
    "    \n",
    "#load highly_predicted data\n",
    "#index_h, image_h, label_h = get_dataset(128, 128, highly_predict=True)\n",
    "#%%\n",
    "#Sort thhe image\n",
    "index_sort = np.argsort(index)\n",
    "image = image[index_sort]\n",
    "label = label[index_sort]\n",
    "\n",
    "#index_sort_h = np.argsort(index_h)\n",
    "#image_h = image_h[index_sort_h]\n",
    "#label_h = label_h[index_sort_h]\n",
    "#%%\n",
    "label_onehot = np.zeros((label .size, label .max() + 1))\n",
    "label_onehot[np.arange(label .size), label] = 1\n",
    "\n",
    "#label_onehot_h = np.zeros((label_h .size, label_h .max() + 1))\n",
    "#label_onehot_h[np.arange(label_h .size), label_h] = 1\n",
    "#%%\n",
    "#normalize the data\n",
    "def norm(x):\n",
    "    return x/255.\n",
    "\n",
    "image_norm = norm(image)\n",
    "#image_norm_h = norm(image_h)\n",
    "#%%    \n",
    "#seperate picture to train and test\n",
    "x_train = image_norm[0:train_number]\n",
    "y_train = label_onehot[0:train_number]\n",
    "\n",
    "x_test = image_norm[train_number:]\n",
    "y_test = label_onehot[train_number:]\n",
    "\n",
    "#x_train_h = image_h[0:1500]\n",
    "#y_train_h = label_onehot_h[0:1500]\n",
    "\n",
    "#x_test_h = image_h[1500:]\n",
    "#y_test_h = label_onehot_h[1500:]\n",
    "#%%\n",
    "def simple_CNN(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Conv2D(32,(3,3), strides = (1,1))(X_input)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64,(3,3), strides = (1,1))(X_input)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Dense(2, activation = 'softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name = 'baseline')\n",
    "    \n",
    "    return model\n",
    "#%%\n",
    "def deep_CNN(SHAPE, seed=None):\n",
    "    # We can't use ResNet50 directly, as it might cause a negative dimension\n",
    "    # error.\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    input_layer = Input(shape=SHAPE)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv2D(32, (3, 3), init='glorot_uniform',border_mode='same')(input_layer)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv2D(48, (3, 3), init='glorot_uniform', border_mode='same')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv2D(64, (3, 3), init='glorot_uniform', border_mode='same')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv2D(96, (3, 3), init='glorot_uniform', border_mode='same')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Step 3 - Flattening\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "\n",
    "    x = Dense(output_dim=256)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = ReLU()(x)\n",
    "    # Dropout\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(output_dim=2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "#%%\n",
    "def VGG16(SHAPE, seed=None):\n",
    "    # We can't use ResNet50 directly, as it might cause a negative dimension\n",
    "    # error.\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    input_layer = Input(shape=SHAPE)\n",
    "\n",
    "    # block 1\n",
    "    x = Conv2D(64, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block1_conv1')(input_layer)\n",
    "    x = Conv2D(64, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # block 2\n",
    "    x = Conv2D(128, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # block 3\n",
    "    x = Conv2D(256, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name='block5_conv3')(x)\n",
    "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(1, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "\n",
    "    return model      \n",
    "        \n",
    "#%%\n",
    "#train\n",
    "model = deep_CNN(x_train.shape[1:])\n",
    "opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  \n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = 100, batch_size=16)\n",
    "#%%\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "\n",
    "plt.figure()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Training loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "#%%\n",
    "loss, test_accuracy = model.evaluate(x_test, y_test, batch_size = 1, verbose=1)\n",
    "#%%\n",
    "#test\n",
    "y_pred = model.predict(x_test, batch_size = 1, verbose=1)\n",
    "\n",
    "y_pred[y_pred>0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
